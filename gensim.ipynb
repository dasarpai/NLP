{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tutorialspoint.com/gensim/gensim_introduction.htm#:~:text=Gensim%20%3D%20%E2%80%9CGenerate%20Similar%E2%80%9D%20is,Performing%20topic%20identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pandas\n",
    "#!pip install nltk\n",
    "#conda install -c conda-forge textblob\n",
    "#!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gensim = “Generate Similar” is a popular open source natural language processing (NLP) library used for unsupervised topic modeling. It uses top academic models and modern statistical machine learning to perform various complex tasks such as −\n",
    "\n",
    "#Building document or word vectors\n",
    "#Corpora\n",
    "#Performing topic identification\n",
    "#Performing document comparison (retrieving semantically similar documents)\n",
    "#Analysing plain-text documents for semantic structure\n",
    "\n",
    "# In order to speed up processing and retrieval on machine clusters, Gensim provides efficient multicore implementations of various popular algorithms like \n",
    "# Latent Semantic Analysis (LSA), \n",
    "# Latent Dirichlet Allocation (LDA),\n",
    "# Random Projections (RP), \n",
    "# Hierarchical Dirichlet Process (HDP).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim\n",
    "#pip install --upgrade gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['survey', 'user', 'opinion', 'computer', 'system', 'response', 'time'],\n",
      " ['relation', 'user', 'perceived', 'response', 'time', 'error', 'measurement'],\n",
      " ['generation', 'random', 'binary', 'unordered', 'trees'],\n",
      " ['intersection', 'graph', 'paths', 'trees'],\n",
      " ['graph', 'minors', 'iv', 'widths', 'trees', 'well', 'quasi', 'ordering']]\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "t_corpus = [\n",
    "   \"A survey of user opinion of computer system response time\", \n",
    "   \"Relation of user perceived response time to error measurement\", \n",
    "   \"The generation of random binary unordered trees\", \n",
    "   \"The intersection graph of paths in trees\", \n",
    "   \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "]\n",
    "stoplist = set('for a of the and to in'.split(' '))\n",
    "processed_corpus = [[word for word in document.lower().split() if word not in stoplist]\n",
    "   for document in t_corpus]\n",
    "\t\n",
    "pprint.pprint(processed_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['survey',\n",
       " 'of',\n",
       " 'user',\n",
       " 'opinion',\n",
       " 'of',\n",
       " 'computer',\n",
       " 'system',\n",
       " 'response',\n",
       " 'time']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "gensim.utils.simple_preprocess(t_corpus[0], deacc=False, min_len=2, max_len=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<25 unique tokens: ['computer', 'opinion', 'response', 'survey', 'system']...>\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'binary': 11,\n",
      " 'computer': 0,\n",
      " 'error': 7,\n",
      " 'generation': 12,\n",
      " 'graph': 16,\n",
      " 'intersection': 17,\n",
      " 'iv': 19,\n",
      " 'measurement': 8,\n",
      " 'minors': 20,\n",
      " 'opinion': 1,\n",
      " 'ordering': 21,\n",
      " 'paths': 18,\n",
      " 'perceived': 9,\n",
      " 'quasi': 22,\n",
      " 'random': 13,\n",
      " 'relation': 10,\n",
      " 'response': 2,\n",
      " 'survey': 3,\n",
      " 'system': 4,\n",
      " 'time': 5,\n",
      " 'trees': 14,\n",
      " 'unordered': 15,\n",
      " 'user': 6,\n",
      " 'well': 23,\n",
      " 'widths': 24}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)],\n",
      " [(2, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1)],\n",
      " [(11, 1), (12, 1), (13, 1), (14, 1), (15, 1)],\n",
      " [(14, 1), (16, 1), (17, 1), (18, 1)],\n",
      " [(14, 1), (16, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1)]]\n"
     ]
    }
   ],
   "source": [
    "BoW_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\n",
    "pprint.pprint(BoW_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(14, 0.4869354917707381), (16, 0.8734379353188121)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "tfidf = models.TfidfModel(BoW_corpus)\n",
    "words = \"trees graph\".lower().split()\n",
    "print(tfidf[dictionary.doc2bow(words)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = similarities.SparseMatrixSimilarity(tfidf[BoW_corpus],num_features=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_document = 'trees system'.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_bow = dictionary.doc2bow(query_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, 1), (14, 1)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simils = index[tfidf[query_bow]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(list(enumerate(simils)))\n",
    "\n",
    "#for doc_number, score in sorted(enumerate(sims), key=lambda x: x[1], reverse=True):\n",
    "#   print(doc_number, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model=models.LdaModel(corpus, id2word=dictionary, num_topics=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary<27 unique tokens: ['CNTK', 'Computational', 'Network', 'Toolkit', 'as']...>\n"
     ]
    }
   ],
   "source": [
    "doc = [\n",
    "   \"CNTK formerly known as Computational Network Toolkit\",\n",
    "   \"is a free easy-to-use open-source commercial-grade toolkit\",\n",
    "   \"that enable us to train deep learning algorithms to learn like the human brain.\"\n",
    "]\n",
    "\n",
    "text_tokens = [[text for text in doc.split()] for doc in doc]\n",
    "\n",
    "dict_LoS = corpora.Dictionary(text_tokens)\n",
    "\n",
    "print(dict_LoS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'CNTK': 0, 'Computational': 1, 'Network': 2, 'Toolkit': 3, 'as': 4, 'formerly': 5, 'known': 6, 'a': 7, 'commercial-grade': 8, 'easy-to-use': 9, 'free': 10, 'is': 11, 'open-source': 12, 'toolkit': 13, 'algorithms': 14, 'brain.': 15, 'deep': 16, 'enable': 17, 'human': 18, 'learn': 19, 'learning': 20, 'like': 21, 'that': 22, 'the': 23, 'to': 24, 'train': 25, 'us': 26}\n"
     ]
    }
   ],
   "source": [
    "print(dict_LoS.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'as': 0, 'cntk': 1, 'computational': 2, 'formerly': 3, 'known': 4, 'network': 5, 'toolkit': 6, 'commercial': 7, 'easy': 8, 'free': 9, 'grade': 10, 'is': 11, 'open': 12, 'source': 13, 'to': 14, 'use': 15, 'algorithms': 16, 'brain': 17, 'deep': 18, 'enable': 19, 'human': 20, 'learn': 21, 'learning': 22, 'like': 23, 'that': 24, 'the': 25, 'train': 26, 'us': 27}\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "from gensim.utils import simple_preprocess\n",
    "from smart_open import smart_open\n",
    "import os\n",
    "\n",
    "dict_STF = corpora.Dictionary(\n",
    "   simple_preprocess(line, deacc =True) for line in open('./data/doc.txt', encoding='utf-8')\n",
    ")\n",
    "\n",
    "print(dict_STF.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Read_files(object):\n",
    "    def __init__(self, directoryname):\n",
    "        self.directoryname = directoryname\n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.directoryname):\n",
    "            for line in open(os.path.join(self.directoryname, fname), encoding='latin'):\n",
    "                yield simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'as': 0, 'cntk': 1, 'computational': 2, 'formerly': 3, 'known': 4, 'network': 5, 'toolkit': 6, 'commercial': 7, 'easy': 8, 'free': 9, 'grade': 10, 'is': 11, 'open': 12, 'source': 13, 'to': 14, 'use': 15, 'algorithms': 16, 'brain': 17, 'deep': 18, 'enable': 19, 'human': 20, 'learn': 21, 'learning': 22, 'like': 23, 'that': 24, 'the': 25, 'train': 26, 'us': 27}\n"
     ]
    }
   ],
   "source": [
    "path = \"data\"\n",
    "dict_MUL = corpora.Dictionary(Read_files(path))\n",
    "print(dict_MUL.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pprint\n",
    "from gensim import corpora\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_list = [\n",
    "\"Hello, how are you?\", \"How do you do?\",\n",
    "\"Hey what are you doing? yes you What are you doing?\"\n",
    "]\n",
    "\n",
    "doc_tokenized = [simple_preprocess(doc) for doc in doc_list]\n",
    "\n",
    "dictionary = corpora.Dictionary()\n",
    "\n",
    "BoW_corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in doc_tokenized]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('are', 1), ('hello', 1), ('how', 1), ('you', 1)], [('how', 1), ('you', 1), ('do', 2)], [('are', 2), ('you', 3), ('doing', 2), ('hey', 1), ('what', 2), ('yes', 1)]]\n"
     ]
    }
   ],
   "source": [
    "id_words = [[(dictionary[id], count) for id, count in line] for line in BoW_corpus]\n",
    "print(id_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1)], [(2, 1), (3, 1), (4, 2)], [(0, 2), (3, 3), (5, 2), (6, 1), (7, 2), (8, 1)]]\n",
      "[[('are', 1), ('hello', 1), ('how', 1), ('you', 1)], [('how', 1), ('you', 1), ('do', 2)], [('are', 2), ('you', 3), ('doing', 2), ('hey', 1), ('what', 2), ('yes', 1)]]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import pprint\n",
    "from gensim import corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "doc_list = [\n",
    "   \"Hello, how are you?\", \"How do you do?\", \n",
    "   \"Hey what are you doing? yes you What are you doing?\"\n",
    "]\n",
    "doc_tokenized = [simple_preprocess(doc) for doc in doc_list]\n",
    "dictionary = corpora.Dictionary()\n",
    "BoW_corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in doc_tokenized]\n",
    "print(BoW_corpus)\n",
    "id_words = [[(dictionary[id], count) for id, count in line] for line in BoW_corpus]\n",
    "print(id_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)], [(6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1)], [(14, 2), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1)]]\n"
     ]
    }
   ],
   "source": [
    "doc_tokenized = [\n",
    "   simple_preprocess(line, deacc =True) for line in open(path+'/doc.txt', encoding='utf-8')\n",
    "]\n",
    "dictionary = corpora.Dictionary()\n",
    "\n",
    "\n",
    "BoW_corpus = [\n",
    "   dictionary.doc2bow(doc, allow_update=True) for doc in doc_tokenized\n",
    "]\n",
    "print(BoW_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)], [(6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1)], [(14, 2), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1)]]\n"
     ]
    }
   ],
   "source": [
    "doc_tokenized = [\n",
    "   simple_preprocess(line, deacc =True) for line in open('./data/doc.txt', encoding='utf-8')\n",
    "]\n",
    "dictionary = corpora.Dictionary()\n",
    "BoW_corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in doc_tokenized]\n",
    "print(BoW_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0.7071067811865475), (3, 0.7071067811865475)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tfidf = models.TfidfModel(BoW_corpus)\n",
    "\n",
    "doc_BoW = [(1,1),(3,1)]\n",
    "print(tfidf[doc_BoW])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.40369167389095173), (1, 0.40369167389095173), (2, 0.40369167389095173), (3, 0.40369167389095173), (4, 0.40369167389095173), (5, 0.40369167389095173), (6, 0.1489905855640844)]\n",
      "[(6, 0.12831948188497175), (7, 0.34768308506769946), (8, 0.34768308506769946), (9, 0.34768308506769946), (10, 0.34768308506769946), (11, 0.34768308506769946), (12, 0.34768308506769946), (13, 0.34768308506769946), (14, 0.12831948188497175), (15, 0.34768308506769946)]\n",
      "[(14, 0.20840410544601642), (16, 0.2823366384349904), (17, 0.2823366384349904), (18, 0.2823366384349904), (19, 0.2823366384349904), (20, 0.2823366384349904), (21, 0.2823366384349904), (22, 0.2823366384349904), (23, 0.2823366384349904), (24, 0.2823366384349904), (25, 0.2823366384349904), (26, 0.2823366384349904), (27, 0.2823366384349904)]\n"
     ]
    }
   ],
   "source": [
    "corpus_tfidf = tfidf[BoW_corpus]\n",
    "for doc in corpus_tfidf:\n",
    "   print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model=models.TfidfModel(corpus_tfidf, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model=models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model=models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model=models.RpModel(corpus_tfidf, num_topics=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model=models.HdpModel(corpus_tfidf, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['are', 1], ['hello', 1], ['how', 1], ['you', 1]]\n",
      "[['how', 1], ['you', 1], ['do', 2]]\n",
      "[['are', 2], ['you', 3], ['doing', 2], ['hey', 1], ['what', 2], ['yes', 1]]\n",
      "[['are', 0.4], ['hello', 0.81], ['how', 0.4], ['you', 0.17]]\n",
      "[['how', 0.24], ['you', 0.1], ['do', 0.97]]\n",
      "[['are', 0.3], ['you', 0.18], ['doing', 0.59], ['hey', 0.3], ['what', 0.59], ['yes', 0.3]]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import pprint\n",
    "from gensim import corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "doc_list = [\n",
    "   \"Hello, how are you?\", \"How do you do?\", \n",
    "   \"Hey what are you doing? yes you What are you doing?\"\n",
    "]\n",
    "doc_tokenized = [simple_preprocess(doc) for doc in doc_list]\n",
    "dictionary = corpora.Dictionary()\n",
    "BoW_corpus = [dictionary.doc2bow(doc, allow_update=True) for doc in doc_tokenized]\n",
    "for doc in BoW_corpus:\n",
    "   print([[dictionary[id], freq] for id, freq in doc])\n",
    "import numpy as np\n",
    "tfidf = models.TfidfModel(BoW_corpus, smartirs='ntc')\n",
    "for doc in tfidf[BoW_corpus]:\n",
    "   print([[dictionary[id], np.around(freq,decimals=2)] for id, freq in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model=models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=100)\n",
    "#Model=models.HdpModel(corpus_tfidf, id2word=dictionary)\n",
    "#Model=models.HdpModel(corpus_tfidf, id2word=dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk;\n",
    "nltk.download('stopwords')\n",
    "nlp = spacy.load('en_core_web_md', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#newsgroups_train.data[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pyLDAvis' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6640/1699160983.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'pyLDAvis' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.5.2-cp37-cp37m-win_amd64.whl (7.2 MB)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-9.1.1-cp37-cp37m-win_amd64.whl (3.3 MB)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in d:\\users\\admin\\anaconda3\\envs\\sanskrit\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\users\\admin\\anaconda3\\envs\\sanskrit\\lib\\site-packages (from matplotlib) (1.21.6)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in d:\\users\\admin\\anaconda3\\envs\\sanskrit\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.33.3-py3-none-any.whl (930 kB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.2-cp37-cp37m-win_amd64.whl (54 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\users\\admin\\anaconda3\\envs\\sanskrit\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: typing-extensions in d:\\users\\admin\\anaconda3\\envs\\sanskrit\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib) (3.7.4.3)\n",
      "Requirement already satisfied: six>=1.5 in d:\\users\\admin\\anaconda3\\envs\\sanskrit\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n",
      "Installing collected packages: pillow, kiwisolver, fonttools, cycler, matplotlib\n",
      "Successfully installed cycler-0.11.0 fonttools-4.33.3 kiwisolver-1.4.2 matplotlib-3.5.2 pillow-9.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import spacy\n",
    "#import pyLDAvis\n",
    "#import pyLDAvis.gensim\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "data = newsgroups_train.data\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "#print(data[:4]) #it will print the data after prepared for stopwords\n",
    "bigram = gensim.models.Phrases(data, min_count=5, threshold=100)\n",
    "trigram = gensim.models.Phrases(bigram[data], threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "   return [[word for word in simple_preprocess(str(doc)) \n",
    "   if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "   return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "   [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "   texts_out = []\n",
    "   for sent in texts:\n",
    "      doc = nlp(\" \".join(sent))\n",
    "      texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "   return texts_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['s', 'thing', 'car', 'nntp', 'post', 'host', 'park', 'line', 'wonder', 'enlighten', 'car', 'see', 'day', 'door', 'sport', 'car', 'look', 'late', 'early', 'call', 'bricklin', 'door', 'really', 'small', 'addition', 'front', 'bumper', 'separate', 'rest', 'body', 'know', 'tellme', 'model', 'name', 'engine', 'spec', 'year', 'production', 'car', 'make', 'history', 'info', 'funky', 'look', 'car', 'mail', 'thank', 'bring', 'neighborhood', 'lerxst'], ['si', 'clock', 'poll', 'final', 'call', 'summary', 'final', 'call', 'si', 'clock', 'report', 'keyword', 'si', 'acceleration', 'clock', 'upgrade', 'article', 'shelley', 'line', 'nntp', 'post', 'host', 'fair', 'number', 'brave', 'soul', 'upgrade', 'si', 'clock', 'oscillator', 'share', 'experience', 'poll', 'send', 'brief', 'message', 'detailing', 'experience', 'procedure', 'top', 'speed', 'attain', 'cpu', 'rate', 'speed', 'add', 'card', 'adapter', 'heat', 'sink', 'hour', 'usage', 'day', 'floppy', 'disk', 'functionality', 'floppy', 'especially', 'request', 'summarize', 'next', 'day', 'add', 'network', 'knowledge', 'base', 'do', 'clock', 'upgrade', 'answer', 'poll', 'thank', 'kuo'], ['question', 'engineering', 'computer', 'network', 'distribution', 'usa', 'line', 'folk', 'mac', 'finally', 'give', 'ghost', 'weekend', 'start', 'life', 'way', 'back', 'sooo', 'm', 'market', 'new', 'machine', 'bit', 'soon', 'intend', 'm', 'look', 'pick', 'powerbook', 'maybe', 'bunch', 'question', 'hopefully', 'answer', 'know', 'dirt', 'next', 'round', 'powerbook', 'introduction', 'expect', 'd', 'hear', 'suppose', 'make', 'appearence', 'summer', 'hear', 'anymore', 'access', 'macleak', 'wonder', 'info', 'hear', 'rumor', 'price', 'drop', 'powerbook', 'line', 'one', 'duo', 'go', 'recently', 's', 'impression', 'display', 'probably', 'swing', 'get', 'disk', 'rather', 'really', 'feel', 'much', 'well', 'display', 'yea', 'look', 'great', 'store', 'really', 'good', 'solicit', 'opinion', 'people', 'day', 'day', 'worth', 'take', 'disk', 'size', 'money', 'hit', 'get', 'active', 'display', 'realize', 'real', 'subjective', 'question', 've', 'play', 'machine', 'computer', 'store', 'figure', 'opinion', 'actually', 'use', 'machine', 'daily', 'prove', 'helpful', 'well', 'hellcat', 'perform', 'thank', 'bunch', 'advance', 'info', 'email', 'ill', 'post', 'summary', 'news', 'read', 'time', 'premium', 'final', 'electrical', 'engineering', 'conviction', 'dangerous', 'enemy', 'truth', 'lie'], ['system', 'division', 'line', 'distribution', 'world', 'nntp', 'post', 'host', 'newsreader', 'tin', 'version', 'write', 'write', 'article', 'know', 'weitek', 'far', 'low', 'level', 'stuff', 'go', 'look', 'pretty', 'nice', 'get', 'quadrilateral', 'fill', 'command', 'require', 'point', 'weitek', 'address', 'phone', 'number', 'd', 'get', 'information', 'chip', 'computer', 'system', 'division', 'thing', 'really', 'scare', 'person', 'sense', 'humor', 'winter']]\n",
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 5), (7, 1), (8, 2), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 2), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 1)], [(5, 2), (7, 2), (15, 1), (20, 1), (27, 1), (29, 1), (40, 1), (44, 1), (45, 1), (46, 2), (47, 1), (48, 1), (49, 1), (50, 1), (51, 1), (52, 1), (53, 1), (54, 5), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 2), (61, 1), (62, 2), (63, 2), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 1), (73, 1), (74, 1), (75, 3), (76, 1), (77, 1), (78, 1), (79, 1), (80, 1), (81, 1), (82, 1), (83, 4), (84, 1), (85, 1), (86, 2), (87, 1), (88, 1), (89, 1), (90, 3), (91, 1)], [(7, 2), (16, 2), (17, 1), (20, 2), (21, 2), (23, 1), (29, 1), (31, 2), (33, 1), (40, 1), (42, 1), (47, 1), (57, 2), (62, 1), (71, 1), (72, 1), (88, 1), (92, 1), (93, 1), (94, 1), (95, 1), (96, 1), (97, 1), (98, 1), (99, 1), (100, 2), (101, 2), (102, 1), (103, 1), (104, 1), (105, 1), (106, 1), (107, 3), (108, 1), (109, 1), (110, 1), (111, 1), (112, 1), (113, 1), (114, 2), (115, 1), (116, 1), (117, 1), (118, 1), (119, 1), (120, 2), (121, 1), (122, 1), (123, 1), (124, 1), (125, 1), (126, 3), (127, 1), (128, 1), (129, 1), (130, 1), (131, 1), (132, 1), (133, 1), (134, 1), (135, 1), (136, 1), (137, 2), (138, 1), (139, 3), (140, 1), (141, 1), (142, 1), (143, 1), (144, 1), (145, 1), (146, 1), (147, 1), (148, 2), (149, 1), (150, 1), (151, 1), (152, 1), (153, 3), (154, 1), (155, 1), (156, 1), (157, 1), (158, 3), (159, 1), (160, 1), (161, 1), (162, 1), (163, 1), (164, 1), (165, 1), (166, 1), (167, 1), (168, 1), (169, 1), (170, 1), (171, 2), (172, 1), (173, 1), (174, 1), (175, 1), (176, 1), (177, 1), (178, 1), (179, 1), (180, 1), (181, 1), (182, 1), (183, 1), (184, 2), (185, 1), (186, 1)], [(15, 1), (17, 1), (20, 1), (21, 1), (27, 1), (29, 1), (31, 1), (41, 1), (48, 1), (73, 1), (101, 1), (103, 1), (108, 1), (120, 2), (123, 1), (187, 1), (188, 1), (189, 1), (190, 2), (191, 1), (192, 1), (193, 1), (194, 1), (195, 1), (196, 1), (197, 1), (198, 1), (199, 1), (200, 1), (201, 1), (202, 1), (203, 1), (204, 1), (205, 1), (206, 1), (207, 1), (208, 2), (209, 1), (210, 1), (211, 2), (212, 1), (213, 1), (214, 2)]]\n"
     ]
    }
   ],
   "source": [
    "data_words_nostops = remove_stopwords(data)\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "nlp = spacy.load('en_core_web_md', disable=['parser', 'ner'])\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=[\n",
    "   'NOUN', 'ADJ', 'VERB', 'ADV'\n",
    "])\n",
    "print(data_lemmatized[:4]) #it will print the lemmatized data.\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "texts = data_lemmatized\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "print(corpus[:4]) #it will print the corpus we created above.\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:4]] \n",
    "#it will print the words with their frequencies.\n",
    "lda_model = gensim.models.ldamodel.LdaModel(\n",
    "   corpus=corpus, id2word=id2word, num_topics=20, random_state=100, \n",
    "   update_every=1, chunksize=100, passes=10, alpha='auto', per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]\n",
    "\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))\n",
    "\n",
    "coherence_model_lda = CoherenceModel(\n",
    "   model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v'\n",
    ")\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n",
    "\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "os.environ.update({'MALLET_HOME':r'C:/mallet-2.0.8/'}) \n",
    "#You should update this path as per the path of Mallet directory on your system.\n",
    "mallet_path = r'C:/mallet-2.0.8/bin/mallet' \n",
    "#You should update this path as per the path of Mallet directory on your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamallet = gensim.models.wrappers.LdaMallet(\n",
    "   mallet_path, corpus=corpus, num_topics=20, id2word=id2word\n",
    ")\n",
    "pprint(ldamallet.show_topics(formatted=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamallet = gensim.models.wrappers.LdaMallet(\n",
    "   mallet_path, corpus=corpus, num_topics=20, id2word=id2word\n",
    ")\n",
    "pprint(ldamallet.show_topics(formatted=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coherence_values_computation(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(\n",
    "            mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word\n",
    "        )\n",
    "        model_list.append(model)\n",
    "    coherencemodel = CoherenceModel(\n",
    "        model=model, texts=texts, dictionary=dictionary, coherence='c_v'\n",
    "    )\n",
    "    coherence_values.append(coherencemodel.get_coherence())\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coherence_values = coherence_values_computation (\n",
    "   dictionary=id2word, corpus=corpus, texts=data_lemmatized, \n",
    "   start=1, limit=50, step=8\n",
    ")\n",
    "limit=50; start=1; step=8;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m, cv in zip(x, coherence_values):\n",
    "   print(\"Num Topics =\", m, \" is having Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_model = model_list[3]\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding dominant topics in sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dominant_topics(ldamodel=lda_model, corpus=corpus, texts=data):\n",
    "sent_topics_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in enumerate(ldamodel[corpus]):\n",
    "    row = sorted(row, key=lambda x: (x[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, (topic_num, prop_topic) in enumerate(row):\n",
    "    if j == 0: # => dominant topic\n",
    "        wp = ldamodel.show_topic(topic_num)\n",
    "        topic_keywords = \", \".join([word for word, prop in wp])\n",
    "        sent_topics_df = sent_topics_df.append(\n",
    "        pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "    else:\n",
    "        break\n",
    "    \n",
    "sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = pd.Series(texts)\n",
    "sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "   return(sent_topics_df)\n",
    "df_topic_sents_keywords = dominant_topics(\n",
    "   ldamodel=optimal_model, corpus=corpus, texts=data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = [\n",
    "'Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coherence_values_computation(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "   coherence_values = []\n",
    "   model_list = []\n",
    "   for num_topics in range(start, limit, step):\n",
    "      model = gensim.models.wrappers.LdaMallet(\n",
    "         mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word\n",
    "      )\n",
    "      model_list.append(model)\n",
    "   coherencemodel = CoherenceModel(\n",
    "      model=model, texts=texts, dictionary=dictionary, coherence='c_v'\n",
    "   )\n",
    "   coherence_values.append(coherencemodel.get_coherence())\n",
    "return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coherence_values = coherence_values_computation (\n",
    "   dictionary=id2word, corpus=corpus, texts=data_lemmatized, \n",
    "   start=1, limit=50, step=8\n",
    ")\n",
    "limit=50; start=1; step=8;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m, cv in zip(x, coherence_values):\n",
    "   print(\"Num Topics =\", m, \" is having Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_model = model_list[3]\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dominant_topics(ldamodel=lda_model, corpus=corpus, texts=data):\n",
    "    sent_topics_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in enumerate(ldamodel[corpus]):\n",
    "    row = sorted(row, key=lambda x: (x[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j, (topic_num, prop_topic) in enumerate(row):\n",
    "   if j == 0: # => dominant topic\n",
    "      wp = ldamodel.show_topic(topic_num)\n",
    "      topic_keywords = \", \".join([word for word, prop in wp])\n",
    "sent_topics_df = sent_topics_df.append(\n",
    "   pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True\n",
    ")\n",
    "   else:\n",
    "      break\n",
    "sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = pd.Series(texts)\n",
    "   sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "   return(sent_topics_df)\n",
    "df_topic_sents_keywords = dominant_topics(\n",
    "   ldamodel=optimal_model, corpus=corpus, texts=data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = [\n",
    "'Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Most Representative Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "   sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet,\n",
    "grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], axis=0)\n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "sent_topics_sorteddf_mallet.columns = [\n",
    "   'Topic_Number', \"Contribution_Perc\", \"Keywords\", \"Text\"\n",
    "]\n",
    "sent_topics_sorteddf_mallet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Volume & Distribution of Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topics = pd.concat(\n",
    "[topic_num_keywords, topic_counts, topic_contribution], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dominant_topics.columns = [\n",
    "   'Dominant-Topic', 'Topic-Keywords', 'Num_Documents', 'Perc_Documents'\n",
    "]\n",
    "df_dominant_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSI & HDP Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "data = newsgroups_train.data\n",
    "data = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in data]\n",
    "data = [re.sub('\\s+', ' ', sent) for sent in data]\n",
    "data = [re.sub(\"\\'\", \"\", sent) for sent in data]\n",
    "\n",
    "#print(data_words[:4]) #it will print the data after prepared for stopwords\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100)\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "   return [[word for word in simple_preprocess(str(doc)) \n",
    "   if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "   return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "   return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "   texts_out = []\n",
    "   for sent in texts:\n",
    "      doc = nlp(\" \".join(sent))\n",
    "      texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "   return texts_out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words_nostops = remove_stopwords(data_words)\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "nlp = spacy.load('en_core_web_md', disable=['parser', 'ner'])\n",
    "data_lemmatized = lemmatization(\n",
    "   data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']\n",
    ")\n",
    "print(data_lemmatized[:4]) #it will print the lemmatized data.\n",
    "\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "texts = data_lemmatized\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "print(corpus[:4]) #it will print the corpus we created above.\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:4]] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#it will print the words with their frequencies.\n",
    "lsi_model = gensim.models.lsimodel.LsiModel(\n",
    "   corpus=corpus, id2word=id2word, num_topics=20,chunksize=100\n",
    ")\n",
    "\n",
    "pprint(lsi_model.print_topics())\n",
    "doc_lsi = lsi_model[corpus]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hdp_model = gensim.models.hdpmodel.HdpModel(corpus=corpus, id2word=id2word)\n",
    "pprint(Hdp_model.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Developing Word2Vec Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = [\n",
    "   ['this', 'is', 'gensim', 'tutorial', 'for', 'free'],\n",
    "   ['this', 'is', 'the', 'tutorials' 'point', 'website'],\n",
    "   ['you', 'can', 'read', 'technical','tutorials', 'for','free'],\n",
    "   ['we', 'are', 'implementing','word2vec'],\n",
    "   ['learn', 'full', 'gensim', 'tutorial']\n",
    "]\n",
    "model = Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec<vocab=20, vector_size=100, alpha=0.025>\n",
      "['this', 'is', 'gensim', 'tutorial', 'for', 'free', 'learn', 'the', 'tutorialspoint', 'website', 'full', 'can', 'read', 'technical', 'tutorials', 'we', 'are', 'implementing', 'word2vec', 'you']\n",
      "Word2Vec<vocab=20, vector_size=100, alpha=0.025>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(model)\n",
    "words = list(model.wv.key_to_index.keys())\n",
    "print(words)\n",
    "\n",
    "#print(model['tutorial'])\n",
    "model.save('model.bin')\n",
    "new_model = Word2Vec.load('model.bin')\n",
    "print(new_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Word2Vec(sentences, min_count=1)\n",
    "sent_vec=[]\n",
    "import numpy as np\n",
    "for sent in sentences:\n",
    "    sents=[]\n",
    "    for word in sent:\n",
    "        word_vec = model.wv.get_vector(word)\n",
    "        sents.append(word_vec)\n",
    "    sent_vec.append(np.array(sents).mean(axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PCA' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6640/1539429425.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msent_vec\u001b[0m \u001b[1;31m#model[words]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpca\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpyplot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PCA' is not defined"
     ]
    }
   ],
   "source": [
    "model = Word2Vec(sentences, min_count=1)\n",
    "import PCA\n",
    "X = sent_vec #model[words]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "words = list(model.wv.vocab)\n",
    "for i, word in enumerate(words):\n",
    "    pyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Document Vectors Using Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api\n",
    "dataset = api.load(\"text8\")\n",
    "data = [d for d in dataset]\n",
    "def tagged_document(list_of_list_of_words):\n",
    "   for i, list_of_words in enumerate(list_of_list_of_words):\n",
    "      yield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_training = list(tagged_document(data))\n",
    "#print(data_for_training[:1])\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=40, min_count=2, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.04645032 -0.06525037 -0.44830814  0.0735919  -0.01256045 -0.19441895\n",
      "  0.00240697  0.04283302 -0.08887281  0.06730043  0.08412008 -0.02412098\n",
      " -0.29227853 -0.25381443 -0.25344747  0.09784282 -0.03148327  0.01436284\n",
      " -0.27366793 -0.08353205 -0.09285121  0.06544829 -0.10899279  0.1396095\n",
      " -0.09804571 -0.09945874 -0.15530185 -0.06962255  0.09758286 -0.02589562\n",
      "  0.03024218  0.05621434 -0.36436296 -0.5566834  -0.22413267 -0.21288651\n",
      " -0.12768608 -0.2614937  -0.0357865  -0.11282396]\n"
     ]
    }
   ],
   "source": [
    "model.build_vocab(data_for_training)\n",
    "model.train(data_for_training, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "print(model.infer_vector(['violent', 'means', 'to', 'destroy', 'the','organization'])) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bigger_list=[]\n",
    "for i in df['patterns']:\n",
    "    li = list(i.split(\" \"))\n",
    "    Bigger_list.append(li)\n",
    "Model= Word2Vec(Bigger_list,min_count=1,size=300,workers=4)\n",
    "\n",
    "\n",
    "Model.save(\"word2vec.model\")\n",
    "Model.save(\"model.bin\")\n",
    "\n",
    "model = Word2Vec.load('model.bin')\n",
    "\n",
    "vocab = list(model.wv.vocab)\n",
    "vocab\n",
    "\n",
    "\n",
    "similar_words = model.most_similar('thanks')\n",
    "print(similar_words)\n",
    "\n",
    "\n",
    "dissimlar_words = model.doesnt_match('See you later, thanks for visiting'.split())\n",
    "print(dissimlar_words)\n",
    "\n",
    "\n",
    "similarity_two_words = model.similarity('please','see')\n",
    "print(\"Please provide the similarity between these two words:\")\n",
    "print(similarity_two_words)\n",
    "\n",
    "\n",
    "similar = model.similar_by_word('kind')\n",
    "print(similar)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6ca0fa7ce7bbb0a79954fd344190fa79005e3baa6147c7762ef10e98302bd8b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('sanskrit')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
