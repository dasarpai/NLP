{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inpiration https://towardsdatascience.com/a-simple-explanation-of-the-bag-of-words-model-b88fc4f4971"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   beat  deed  deeds  earthquake  god  good  karma  reason  responsible\n",
      "0     1     0      1           1    1     0      1       1            0\n",
      "1     0     1      0           0    1     1      2       0            1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\admin\\anaconda3\\envs\\sanskrit\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    " \n",
    "sentence_1=\"Our Deeds are the Reason of this earthquake May God beat us more for our Karma.\"\n",
    "sentence_2=\"Our Karma is not good and Karma is nothing but deed. God is not responsible.\"\n",
    " \n",
    " \n",
    " \n",
    "CountVec = CountVectorizer(ngram_range=(1,1), # to use bigrams ngram_range=(2,2)\n",
    "                           stop_words='english')\n",
    "#transform\n",
    "Count_data = CountVec.fit_transform([sentence_1,sentence_2])\n",
    " \n",
    "#create dataframe\n",
    "cv_dataframe=pd.DataFrame(Count_data.toarray(),columns=CountVec.get_feature_names())\n",
    "print(cv_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file='Cleaned-Twitter.csv'\n",
    "df = pd.read_csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Our Deeds are the Reason of this earthquake May ALLAH Forgive us all'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0]['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third',\n",
       "       'this'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "       'This is the first document.',\n",
    "       'This document is the second document.',\n",
    "       'And this is the third one.',\n",
    "       'Is this the first document?',\n",
    "   ]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "vectorizer.get_feature_names_out()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df      = pd.read_csv(file)\n",
    "df_full = pd.read_csv(file)\n",
    "\n",
    "list_corpus      = df[\"text\"].tolist()\n",
    "list_corpus_full = df_full[\"text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_features=300, ngram_range=(1,2) )\n",
    "\n",
    "emb = tfidf_vectorizer.fit_transform(df['text']) \n",
    "\n",
    "def tfidf_vec(data):\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=300, ngram_range=(1,2) )\n",
    "\n",
    "    emb = tfidf_vectorizer.fit_transform(data)\n",
    "\n",
    "    return emb, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(TfidfVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_tfidf, vec_tfidf   = tfidf_vec(list_corpus_full)\n",
    "\n",
    "embed_tfidf  = embed_tfidf.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7610, 300)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.iloc[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.        , 0.        , 0.        , 0.32868119,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.29736785,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.4554877 , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.40650815, 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.1881527 , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.39690819, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.15955002, 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.28261974, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.36408163, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_tfidf[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\admin\\anaconda3\\envs\\sanskrit\\lib\\site-packages\\sklearn\\utils\\validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components=15)\n",
    "embed_tfidf  = pca.fit_transform(embed_tfidf)\n",
    "\n",
    "embed_tfidf          = pd.DataFrame(embed_tfidf, index=df_full.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7610, 15)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.124003</td>\n",
       "      <td>-0.051135</td>\n",
       "      <td>0.099157</td>\n",
       "      <td>-0.015947</td>\n",
       "      <td>0.015310</td>\n",
       "      <td>-0.035803</td>\n",
       "      <td>-0.076231</td>\n",
       "      <td>0.025651</td>\n",
       "      <td>0.017077</td>\n",
       "      <td>-0.003326</td>\n",
       "      <td>-0.003818</td>\n",
       "      <td>-0.002022</td>\n",
       "      <td>-0.018459</td>\n",
       "      <td>-0.006149</td>\n",
       "      <td>-0.029510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.100867</td>\n",
       "      <td>-0.020499</td>\n",
       "      <td>0.064507</td>\n",
       "      <td>-0.071436</td>\n",
       "      <td>-0.053976</td>\n",
       "      <td>-0.036469</td>\n",
       "      <td>0.050117</td>\n",
       "      <td>-0.007321</td>\n",
       "      <td>-0.003451</td>\n",
       "      <td>-0.009432</td>\n",
       "      <td>-0.039783</td>\n",
       "      <td>-0.025142</td>\n",
       "      <td>-0.053898</td>\n",
       "      <td>-0.000768</td>\n",
       "      <td>-0.097997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.078903</td>\n",
       "      <td>0.250932</td>\n",
       "      <td>-0.067893</td>\n",
       "      <td>0.083065</td>\n",
       "      <td>-0.058727</td>\n",
       "      <td>-0.059212</td>\n",
       "      <td>-0.053616</td>\n",
       "      <td>-0.050516</td>\n",
       "      <td>0.046973</td>\n",
       "      <td>-0.070049</td>\n",
       "      <td>0.137217</td>\n",
       "      <td>0.008557</td>\n",
       "      <td>0.080345</td>\n",
       "      <td>0.026153</td>\n",
       "      <td>-0.012518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.091169</td>\n",
       "      <td>0.216799</td>\n",
       "      <td>0.020647</td>\n",
       "      <td>-0.057901</td>\n",
       "      <td>-0.021135</td>\n",
       "      <td>-0.001490</td>\n",
       "      <td>-0.025755</td>\n",
       "      <td>-0.021277</td>\n",
       "      <td>-0.017611</td>\n",
       "      <td>-0.008904</td>\n",
       "      <td>-0.008622</td>\n",
       "      <td>-0.025747</td>\n",
       "      <td>0.002425</td>\n",
       "      <td>-0.006991</td>\n",
       "      <td>-0.096645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.118222</td>\n",
       "      <td>-0.046835</td>\n",
       "      <td>0.059447</td>\n",
       "      <td>-0.040849</td>\n",
       "      <td>-0.010223</td>\n",
       "      <td>-0.022753</td>\n",
       "      <td>-0.039461</td>\n",
       "      <td>0.038031</td>\n",
       "      <td>-0.095381</td>\n",
       "      <td>-0.029771</td>\n",
       "      <td>-0.087953</td>\n",
       "      <td>-0.029807</td>\n",
       "      <td>-0.208687</td>\n",
       "      <td>0.187833</td>\n",
       "      <td>0.230075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.124003 -0.051135  0.099157 -0.015947  0.015310 -0.035803 -0.076231   \n",
       "1 -0.100867 -0.020499  0.064507 -0.071436 -0.053976 -0.036469  0.050117   \n",
       "2 -0.078903  0.250932 -0.067893  0.083065 -0.058727 -0.059212 -0.053616   \n",
       "3 -0.091169  0.216799  0.020647 -0.057901 -0.021135 -0.001490 -0.025755   \n",
       "4 -0.118222 -0.046835  0.059447 -0.040849 -0.010223 -0.022753 -0.039461   \n",
       "\n",
       "         7         8         9         10        11        12        13  \\\n",
       "0  0.025651  0.017077 -0.003326 -0.003818 -0.002022 -0.018459 -0.006149   \n",
       "1 -0.007321 -0.003451 -0.009432 -0.039783 -0.025142 -0.053898 -0.000768   \n",
       "2 -0.050516  0.046973 -0.070049  0.137217  0.008557  0.080345  0.026153   \n",
       "3 -0.021277 -0.017611 -0.008904 -0.008622 -0.025747  0.002425 -0.006991   \n",
       "4  0.038031 -0.095381 -0.029771 -0.087953 -0.029807 -0.208687  0.187833   \n",
       "\n",
       "         14  \n",
       "0 -0.029510  \n",
       "1 -0.097997  \n",
       "2 -0.012518  \n",
       "3 -0.096645  \n",
       "4  0.230075  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_tfidf.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BOW Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_vec(data):\n",
    "    count_vectorizer = CountVectorizer(max_features=300, ngram_range=(1,1) )\n",
    "\n",
    "    emb = count_vectorizer.fit_transform(data)\n",
    "\n",
    "    return emb, count_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7610, 300)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_bow, vec_bow = bow_vec(list_corpus_full)\n",
    "embed_bow = embed_bow.todense()\n",
    "embed_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_bow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Users\\admin\\anaconda3\\envs\\sanskrit\\lib\\site-packages\\sklearn\\utils\\validation.py:598: FutureWarning: np.matrix usage is deprecated in 1.0 and will raise a TypeError in 1.2. Please convert to a numpy array with np.asarray. For more information see: https://numpy.org/doc/stable/reference/generated/numpy.matrix.html\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "pca = PCA(n_components=15)\n",
    "embed_bow = pca.fit_transform(embed_bow)\n",
    "\n",
    "embed_bow = pd.DataFrame(embed_bow, index=df_full.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7610, 15)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_bow.to_csv(r\"\\embedding_bow.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.648325</td>\n",
       "      <td>-0.372389</td>\n",
       "      <td>-0.337110</td>\n",
       "      <td>0.560750</td>\n",
       "      <td>-0.023368</td>\n",
       "      <td>0.213378</td>\n",
       "      <td>-0.021474</td>\n",
       "      <td>-0.100306</td>\n",
       "      <td>-0.039614</td>\n",
       "      <td>0.047266</td>\n",
       "      <td>-0.150941</td>\n",
       "      <td>0.044855</td>\n",
       "      <td>0.005031</td>\n",
       "      <td>0.016368</td>\n",
       "      <td>0.086918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.592873</td>\n",
       "      <td>-0.245450</td>\n",
       "      <td>-0.221353</td>\n",
       "      <td>-0.052827</td>\n",
       "      <td>-0.159176</td>\n",
       "      <td>-0.074328</td>\n",
       "      <td>-0.104647</td>\n",
       "      <td>0.059738</td>\n",
       "      <td>-0.104205</td>\n",
       "      <td>-0.053962</td>\n",
       "      <td>-0.012184</td>\n",
       "      <td>-0.016305</td>\n",
       "      <td>-0.059550</td>\n",
       "      <td>-0.103953</td>\n",
       "      <td>-0.064446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.198350</td>\n",
       "      <td>0.510601</td>\n",
       "      <td>1.830383</td>\n",
       "      <td>-0.052009</td>\n",
       "      <td>-0.374736</td>\n",
       "      <td>0.174252</td>\n",
       "      <td>-0.126010</td>\n",
       "      <td>0.002092</td>\n",
       "      <td>-0.131043</td>\n",
       "      <td>-0.130586</td>\n",
       "      <td>-0.102571</td>\n",
       "      <td>-0.079316</td>\n",
       "      <td>0.575990</td>\n",
       "      <td>0.171815</td>\n",
       "      <td>0.407271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.502374</td>\n",
       "      <td>-0.379008</td>\n",
       "      <td>0.742974</td>\n",
       "      <td>-0.095465</td>\n",
       "      <td>-0.151257</td>\n",
       "      <td>-0.029165</td>\n",
       "      <td>-0.088930</td>\n",
       "      <td>-0.053088</td>\n",
       "      <td>-0.067557</td>\n",
       "      <td>-0.073817</td>\n",
       "      <td>-0.022631</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.008641</td>\n",
       "      <td>-0.051615</td>\n",
       "      <td>0.004334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.556155</td>\n",
       "      <td>-0.166580</td>\n",
       "      <td>-0.200648</td>\n",
       "      <td>-0.019061</td>\n",
       "      <td>-0.033172</td>\n",
       "      <td>-0.064355</td>\n",
       "      <td>0.121216</td>\n",
       "      <td>-0.100040</td>\n",
       "      <td>-0.041033</td>\n",
       "      <td>0.046747</td>\n",
       "      <td>-0.129987</td>\n",
       "      <td>-0.125644</td>\n",
       "      <td>-0.059882</td>\n",
       "      <td>-0.564723</td>\n",
       "      <td>-0.272394</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.648325 -0.372389 -0.337110  0.560750 -0.023368  0.213378 -0.021474   \n",
       "1 -0.592873 -0.245450 -0.221353 -0.052827 -0.159176 -0.074328 -0.104647   \n",
       "2 -0.198350  0.510601  1.830383 -0.052009 -0.374736  0.174252 -0.126010   \n",
       "3 -0.502374 -0.379008  0.742974 -0.095465 -0.151257 -0.029165 -0.088930   \n",
       "4 -0.556155 -0.166580 -0.200648 -0.019061 -0.033172 -0.064355  0.121216   \n",
       "\n",
       "         7         8         9         10        11        12        13  \\\n",
       "0 -0.100306 -0.039614  0.047266 -0.150941  0.044855  0.005031  0.016368   \n",
       "1  0.059738 -0.104205 -0.053962 -0.012184 -0.016305 -0.059550 -0.103953   \n",
       "2  0.002092 -0.131043 -0.130586 -0.102571 -0.079316  0.575990  0.171815   \n",
       "3 -0.053088 -0.067557 -0.073817 -0.022631  0.005500  0.008641 -0.051615   \n",
       "4 -0.100040 -0.041033  0.046747 -0.129987 -0.125644 -0.059882 -0.564723   \n",
       "\n",
       "         14  \n",
       "0  0.086918  \n",
       "1 -0.064446  \n",
       "2  0.407271  \n",
       "3  0.004334  \n",
       "4 -0.272394  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_bow.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set values for various parameters\n",
    "feature_size = 15    # Word vector dimensionality  \n",
    "window_context = 20  # Context window size                                                                                    \n",
    "min_word_count = 1   # Minimum word count                        \n",
    "sample = 1e-3        # Downsample setting for frequent words\n",
    "sg = 1               # skip-gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df= pd.read_csv(file)\n",
    "corpus = list(df['text'])\n",
    "tokenized_corpus = [nltk.word_tokenize(doc) for doc in corpus]\n",
    "df['text_tkn'] = tokenized_corpus\n",
    "\n",
    "\n",
    "#This is required to create word embedding of full dataset\n",
    "df_full =pd.read_csv(file)\n",
    "corpus_full = list(df_full['text'])\n",
    "tokenized_corpus_full = [nltk.word_tokenize(doc) for doc in corpus_full]\n",
    "df_full['sentence_tkn'] = tokenized_corpus_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = Word2Vec(tokenized_corpus, vector_size=feature_size, \n",
    "                                  window=window_context, min_count = min_word_count,\n",
    "                                  sg=sg, sample=sample, epochs=5)\n",
    "vocabulary = set(embedding_model.wv.index_to_key )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_word_vectors(words, model, vocabulary, num_features):\n",
    "    feature_vector = np.zeros((num_features,),dtype=\"float64\")\n",
    "    nwords = 0.\n",
    "    for word in words:\n",
    "        if word in vocabulary: \n",
    "            nwords = nwords + 1.\n",
    "            feature_vector = np.add(feature_vector, model.wv[word])\n",
    "    if nwords:\n",
    "        feature_vector = np.divide(feature_vector, nwords)\n",
    "    return feature_vector\n",
    "\n",
    "def averaged_word_vectorizer(corpus, model, num_features):\n",
    "    \n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get document level embeddings\n",
    "w2v_doc_features = averaged_word_vectorizer(corpus=tokenized_corpus_full, model=embedding_model,\n",
    "                                             num_features=feature_size)\n",
    "df_w2v = pd.DataFrame(w2v_doc_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.463541</td>\n",
       "      <td>0.385192</td>\n",
       "      <td>0.615906</td>\n",
       "      <td>-0.511017</td>\n",
       "      <td>0.451790</td>\n",
       "      <td>0.181643</td>\n",
       "      <td>0.329359</td>\n",
       "      <td>-0.200348</td>\n",
       "      <td>-0.437826</td>\n",
       "      <td>-0.489277</td>\n",
       "      <td>0.321815</td>\n",
       "      <td>0.449759</td>\n",
       "      <td>-0.164279</td>\n",
       "      <td>-0.633994</td>\n",
       "      <td>-0.174536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.556738</td>\n",
       "      <td>0.055004</td>\n",
       "      <td>0.514086</td>\n",
       "      <td>-0.259937</td>\n",
       "      <td>0.654891</td>\n",
       "      <td>-0.443765</td>\n",
       "      <td>0.298363</td>\n",
       "      <td>0.097618</td>\n",
       "      <td>-0.499772</td>\n",
       "      <td>-0.241564</td>\n",
       "      <td>0.571624</td>\n",
       "      <td>0.468145</td>\n",
       "      <td>-0.295232</td>\n",
       "      <td>-0.435764</td>\n",
       "      <td>-0.155590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.586281</td>\n",
       "      <td>0.288485</td>\n",
       "      <td>0.806471</td>\n",
       "      <td>-0.449322</td>\n",
       "      <td>0.576510</td>\n",
       "      <td>0.114640</td>\n",
       "      <td>0.415376</td>\n",
       "      <td>-0.135801</td>\n",
       "      <td>-0.491195</td>\n",
       "      <td>-0.429666</td>\n",
       "      <td>0.416134</td>\n",
       "      <td>0.653691</td>\n",
       "      <td>-0.078418</td>\n",
       "      <td>-0.808905</td>\n",
       "      <td>-0.276356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.662422</td>\n",
       "      <td>0.174502</td>\n",
       "      <td>0.649841</td>\n",
       "      <td>-0.252678</td>\n",
       "      <td>0.693505</td>\n",
       "      <td>-0.198109</td>\n",
       "      <td>0.378553</td>\n",
       "      <td>0.160439</td>\n",
       "      <td>-0.307674</td>\n",
       "      <td>-0.401286</td>\n",
       "      <td>0.544405</td>\n",
       "      <td>0.597655</td>\n",
       "      <td>-0.226909</td>\n",
       "      <td>-0.683569</td>\n",
       "      <td>-0.288481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.585272</td>\n",
       "      <td>0.397375</td>\n",
       "      <td>1.009378</td>\n",
       "      <td>-0.458882</td>\n",
       "      <td>0.497112</td>\n",
       "      <td>-0.045452</td>\n",
       "      <td>0.472325</td>\n",
       "      <td>-0.383114</td>\n",
       "      <td>-0.330647</td>\n",
       "      <td>-0.393039</td>\n",
       "      <td>0.405480</td>\n",
       "      <td>0.712295</td>\n",
       "      <td>-0.071999</td>\n",
       "      <td>-0.623767</td>\n",
       "      <td>0.032724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7605</th>\n",
       "      <td>-0.868954</td>\n",
       "      <td>0.313230</td>\n",
       "      <td>1.242468</td>\n",
       "      <td>-0.350825</td>\n",
       "      <td>0.873842</td>\n",
       "      <td>-0.358923</td>\n",
       "      <td>0.338369</td>\n",
       "      <td>-0.125452</td>\n",
       "      <td>-0.161789</td>\n",
       "      <td>-0.384963</td>\n",
       "      <td>0.648030</td>\n",
       "      <td>0.569826</td>\n",
       "      <td>0.096923</td>\n",
       "      <td>-0.702922</td>\n",
       "      <td>-0.224239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7606</th>\n",
       "      <td>-0.717047</td>\n",
       "      <td>0.381327</td>\n",
       "      <td>0.771373</td>\n",
       "      <td>-0.401854</td>\n",
       "      <td>0.692056</td>\n",
       "      <td>0.041093</td>\n",
       "      <td>0.248938</td>\n",
       "      <td>0.054199</td>\n",
       "      <td>-0.259814</td>\n",
       "      <td>-0.443346</td>\n",
       "      <td>0.737303</td>\n",
       "      <td>0.559491</td>\n",
       "      <td>-0.266619</td>\n",
       "      <td>-0.812245</td>\n",
       "      <td>-0.150675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7607</th>\n",
       "      <td>-0.437679</td>\n",
       "      <td>0.321830</td>\n",
       "      <td>0.026815</td>\n",
       "      <td>-0.422229</td>\n",
       "      <td>0.928994</td>\n",
       "      <td>-0.372119</td>\n",
       "      <td>0.355659</td>\n",
       "      <td>-0.012871</td>\n",
       "      <td>-0.304681</td>\n",
       "      <td>-0.423134</td>\n",
       "      <td>0.316804</td>\n",
       "      <td>0.211306</td>\n",
       "      <td>-0.380352</td>\n",
       "      <td>-0.126631</td>\n",
       "      <td>-0.424571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>-0.421805</td>\n",
       "      <td>0.263361</td>\n",
       "      <td>0.792902</td>\n",
       "      <td>-0.352236</td>\n",
       "      <td>0.573006</td>\n",
       "      <td>-0.296036</td>\n",
       "      <td>0.371975</td>\n",
       "      <td>-0.131402</td>\n",
       "      <td>-0.317929</td>\n",
       "      <td>-0.331936</td>\n",
       "      <td>0.373273</td>\n",
       "      <td>0.645215</td>\n",
       "      <td>-0.103157</td>\n",
       "      <td>-0.651510</td>\n",
       "      <td>-0.217778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>-1.177398</td>\n",
       "      <td>0.249381</td>\n",
       "      <td>0.235386</td>\n",
       "      <td>-0.283408</td>\n",
       "      <td>1.716153</td>\n",
       "      <td>-0.458434</td>\n",
       "      <td>0.167347</td>\n",
       "      <td>0.824062</td>\n",
       "      <td>-0.463981</td>\n",
       "      <td>-0.540047</td>\n",
       "      <td>1.545499</td>\n",
       "      <td>0.383048</td>\n",
       "      <td>-0.633807</td>\n",
       "      <td>-0.358082</td>\n",
       "      <td>-0.562246</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7610 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6   \\\n",
       "0    -0.463541  0.385192  0.615906 -0.511017  0.451790  0.181643  0.329359   \n",
       "1    -0.556738  0.055004  0.514086 -0.259937  0.654891 -0.443765  0.298363   \n",
       "2    -0.586281  0.288485  0.806471 -0.449322  0.576510  0.114640  0.415376   \n",
       "3    -0.662422  0.174502  0.649841 -0.252678  0.693505 -0.198109  0.378553   \n",
       "4    -0.585272  0.397375  1.009378 -0.458882  0.497112 -0.045452  0.472325   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "7605 -0.868954  0.313230  1.242468 -0.350825  0.873842 -0.358923  0.338369   \n",
       "7606 -0.717047  0.381327  0.771373 -0.401854  0.692056  0.041093  0.248938   \n",
       "7607 -0.437679  0.321830  0.026815 -0.422229  0.928994 -0.372119  0.355659   \n",
       "7608 -0.421805  0.263361  0.792902 -0.352236  0.573006 -0.296036  0.371975   \n",
       "7609 -1.177398  0.249381  0.235386 -0.283408  1.716153 -0.458434  0.167347   \n",
       "\n",
       "            7         8         9         10        11        12        13  \\\n",
       "0    -0.200348 -0.437826 -0.489277  0.321815  0.449759 -0.164279 -0.633994   \n",
       "1     0.097618 -0.499772 -0.241564  0.571624  0.468145 -0.295232 -0.435764   \n",
       "2    -0.135801 -0.491195 -0.429666  0.416134  0.653691 -0.078418 -0.808905   \n",
       "3     0.160439 -0.307674 -0.401286  0.544405  0.597655 -0.226909 -0.683569   \n",
       "4    -0.383114 -0.330647 -0.393039  0.405480  0.712295 -0.071999 -0.623767   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "7605 -0.125452 -0.161789 -0.384963  0.648030  0.569826  0.096923 -0.702922   \n",
       "7606  0.054199 -0.259814 -0.443346  0.737303  0.559491 -0.266619 -0.812245   \n",
       "7607 -0.012871 -0.304681 -0.423134  0.316804  0.211306 -0.380352 -0.126631   \n",
       "7608 -0.131402 -0.317929 -0.331936  0.373273  0.645215 -0.103157 -0.651510   \n",
       "7609  0.824062 -0.463981 -0.540047  1.545499  0.383048 -0.633807 -0.358082   \n",
       "\n",
       "            14  \n",
       "0    -0.174536  \n",
       "1    -0.155590  \n",
       "2    -0.276356  \n",
       "3    -0.288481  \n",
       "4     0.032724  \n",
       "...        ...  \n",
       "7605 -0.224239  \n",
       "7606 -0.150675  \n",
       "7607 -0.424571  \n",
       "7608 -0.217778  \n",
       "7609 -0.562246  \n",
       "\n",
       "[7610 rows x 15 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "- Inspiration https://towardsdatascience.com/different-techniques-to-represent-words-as-vectors-word-embeddings-3e4b9ab7ceb4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer or (Document Term Matrix) or BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 1 1 1 0 1 0 0 0 1 0 0 0]\n",
      " [0 0 0 1 0 1 0 1 0 0 0 1 0 1 1 0 0]\n",
      " [1 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 1 1 0 0 0 1 0 1 1 1 0 1 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "sentences = [\n",
    "    'He is playing in the field.',\n",
    "    'He is running towards the football.',\n",
    "    'The football game ended.',\n",
    "    'It started raining while everyone was playing in the field.'\n",
    "]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "sentence_vectors = vectorizer.fit_transform(sentences)\n",
    "\n",
    "print(sentence_vectors.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         1.69314718 0.         0.         1.69314718\n",
      "  1.69314718 1.69314718 0.         1.69314718 0.         0.\n",
      "  0.         1.         0.         0.         0.        ]\n",
      " [0.         0.         0.         1.69314718 0.         1.69314718\n",
      "  0.         1.69314718 0.         0.         0.         2.38629436\n",
      "  0.         1.         2.38629436 0.         0.        ]\n",
      " [2.38629436 0.         0.         1.69314718 2.38629436 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         1.         0.         0.         0.        ]\n",
      " [0.         2.38629436 1.69314718 0.         0.         0.\n",
      "  1.69314718 0.         2.38629436 1.69314718 2.38629436 0.\n",
      "  2.38629436 1.         0.         2.38629436 2.38629436]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "sentences = [\n",
    "    'He is playing in the field.',\n",
    "    'He is running towards the football.',\n",
    "    'The football game ended.',\n",
    "    'It started raining while everyone was playing in the field.'\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer(norm = False, smooth_idf = False)\n",
    "sentence_vectors = vectorizer.fit_transform(sentences)\n",
    "print(sentence_vectors.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hashing Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  0.  0.  1.  0.  0.  0.  1.  0.  0. -1. -1.  0.  0.  1.  0.]\n",
      " [ 1.  1.  0.  0.  1.  0.  0.  0.  1.  0.  0. -2.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0. -1.  0.  0.  0. -1.  0. -1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  1.  1.  1.  0.  0. -1.  0.  0.  0. -1. -1. -2.  0. -1.  1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "sentences = [\n",
    "    'He is playing in the field.',\n",
    "    'He is running towards the football.',\n",
    "    'The football game ended.',\n",
    "    'It started raining while everyone was playing in the field.'\n",
    "]\n",
    "\n",
    "vectorizer = HashingVectorizer(norm = None, n_features = 17)\n",
    "sentence_vectors = vectorizer.fit_transform(sentences)\n",
    "print(sentence_vectors.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "sentences = [\n",
    "    'He is playing in the field.',\n",
    "    'He is running towards the football.',\n",
    "    'The football game ended.',\n",
    "    'It started raining while everyone was playing in the field.'\n",
    "]\n",
    "\n",
    "for i, sentence in enumerate(sentences):\n",
    "\ttokenized= []\n",
    "\tfor word in sentence.split(' '):\n",
    "\t\tword = word.split('.')[0]\n",
    "\t\tword = word.lower()\n",
    "\t\ttokenized.append(word)\n",
    "\tsentences[i] = tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common word to football is: is\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = word2vec.Word2Vec(sentences, workers = 1, vector_size= 2, min_count = 1, window = 3, sg = 0)\n",
    "similar_word = model.wv.most_similar('football')[0]\n",
    "print(\"Most common word to football is: {}\".format(similar_word[0]))\n",
    "\n",
    "## Output\n",
    "# Most common word to football is: game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.11 ('sanskrit')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "d6ca0fa7ce7bbb0a79954fd344190fa79005e3baa6147c7762ef10e98302bd8b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
